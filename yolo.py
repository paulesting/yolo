# -*- coding: utf-8 -*-
"""Yolo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FIzuEAmMQkjWGd8GPnKLILVkNI1ZJfCq
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

!pip install opencv-python
!pip install numpy

!wget https://raw.githubusercontent.com/Jamesrogers221194/AINT-Files/7a1aa267a8b05b133a3b26f95786f12adab45195/AINT515/Practical02/2.png

downloaded = drive.CreateFile({'id':'1RcKTb4-NNgPcT-P6ztANZZ-7sXGMgyMS'})
yolo_weights = downloaded.GetContentFile('yolov3.weights')

downloaded_yolov3 = drive.CreateFile({'id':'1AdFR2mq5x9A3qs9HOfFAzcEBL8TdsvPh'})
yolov3_cfg = downloaded_yolov3.GetContentFile('yolov3.cfg')

downloaded_coco_names = drive.CreateFile({'id':'1kSF1t8fwH-_pc49Qu1rSHrqDbwTLW7m4'})
coco_names= downloaded_coco_names.GetContentFile('coco.names')

downloaded_image = drive.CreateFile({'id':'1QCLyid7-9wb9FBc-dNF-Lr1HaiZQt53X'})
image= downloaded_image.GetContentFile('image.jpg')

import cv2
import numpy as np

yolo = cv2.dnn.readNet("/content/yolov3.weights", "/content/yolov3.cfg")
classes = []

with open("/content/coco.names", "r") as file:
    classes = [line.strip() for line in file.readlines()]
layer_names = yolo.getLayerNames()
output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]

colorRed = (0,0,255)
colorGreen = (0,255,0)

# #Loading Images
name = "/content/image.jpg"
img = cv2.imread(name)
height, width, channels = img.shape

# # Detecting objects
blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)

yolo.setInput(blob)
outputs = yolo.forward(output_layers)

class_ids = []
confidences = []
boxes = []
for output in outputs:
    for detection in output:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:
            center_x = int(detection[0] * width)
            center_y = int(detection[1] * height)
            w = int(detection[2] * width)
            h = int(detection[3] * height)

            x = int(center_x - w / 2)
            y = int(center_y - h / 2)

            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
for i in range(len(boxes)):
    if i in indexes:
        x, y, w, h = boxes[i]
        label = str(classes[class_ids[i]])
        cv2.rectangle(img, (x, y), (x + w, y + h), colorGreen, 3)
        cv2.putText(img, label, (x, y + 10), cv2.FONT_HERSHEY_PLAIN, 8, colorRed, 8)


cv2.imshow("Image_output", img)
cv2.imwrite("output_new.jpg",img)
#cv2.waitKey(0)
#cv2.destroyAllWindows()

#adapting the gradcam

import cv2
import numpy as np

# Load YOLO model and pre-trained weights
yolo = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")

# Load COCO class names
with open("coco.names", "r") as file:
    classes = [line.strip() for line in file.readlines()]

# Get output layer names
layer_names = yolo.getLayerNames()
output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]

# Load input image
name = "image.jpg"
img = cv2.imread(name)
height, width, channels = img.shape

# Detecting objects
blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
yolo.setInput(blob)
outputs = yolo.forward(output_layers)

# Confidence threshold
confidence_threshold = 0.5

class_ids = []
confidences = []
boxes = []
for output in outputs:
    for detection in output:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > confidence_threshold:
            center_x = int(detection[0] * width)
            center_y = int(detection[1] * height)
            w = int(detection[2] * width)
            h = int(detection[3] * height)

            x = int(center_x - w / 2)
            y = int(center_y - h / 2)

            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

# Implement Grad-CAM
for i in range(len(boxes)):
    if i in indexes:
        x, y, w, h = boxes[i]
        # Identify the last convolutional layer
        last_conv_layer = output_layers[-1]
        # Get the output of the last convolutional layer
        conv_outputs = outputs[output_layers.index(last_conv_layer)]

        # Extract the region of interest (ROI) from the feature map
        roi = conv_outputs[class_ids[i]]

        # Normalize the ROI
        roi_min = roi.min()
        roi_max = roi.max()
        normalized_roi = (roi - roi_min) / max(roi_max - roi_min, 1e-5)

        # Resize the normalized ROI to match the size of the detection
        resized_normalized_roi = cv2.resize(normalized_roi, (w, h))

       # Apply a color map to the normalized feature map
        heatmap = cv2.applyColorMap(np.uint8(255 * resized_normalized_roi), cv2.COLORMAP_AUTUMN)

        # Resize the heatmap to match the size of the detection
        heatmap = cv2.resize(heatmap, (w, h))

        # Overlay the heatmap onto the original image
        superimposed_img = np.array(img)
        superimposed_img[y:y+h, x:x+w] = cv2.addWeighted(superimposed_img[y:y+h, x:x+w], 0.3, heatmap, 0.9, 0)

        # Save the Grad-CAM output
        cv2.imshow("output_gradcam_{}.jpg".format(i), superimposed_img)
        cv2.imwrite("output_gradcam_{}.jpg".format(i), superimposed_img)

